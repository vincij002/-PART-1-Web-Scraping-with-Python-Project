{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCRAPING OUR FIRST PAGE WITH REQUESTS:\n",
    "\n",
    "# first we're going to figure out how to download the html of a page that shows the standings for the Italian Seria A \n",
    "# to do that we're going to use the requests library \n",
    "\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next we're going to define the url that we are going to start scraping\n",
    "\n",
    "standings_url = \"https://fbref.com/en/comps/11/Serie-A-Stats\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next we are going to download the page from up above, to do this we are going to use the get method in the requests library.\n",
    "# What this will do is it'll make a request to server and actually download the html of this page \n",
    "\n",
    "\n",
    "data = requests.get(standings_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARSING THE HTML LINKS WITH BEAUTFIULSOUP:\n",
    "\n",
    "# In order to actually parse our html we're going to use a library called beautiful soup:\n",
    "\n",
    "\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next we will initialize it using our html\n",
    "\n",
    "soup = BeautifulSoup(data.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next thing we need to do is give our soup object something to select from the web page. To select the table from the website what\n",
    "# we're going to do is we're going to type: \"soup.select('table.\" this is what we call a css selector. the name of the tag is \"table\"\n",
    "# than add a . than the class name which in this case from the website is stats_table this will select the table elements in our page\n",
    "# that have the class, stats_table than assign it back to a variable called \"standings_table\" This will remove a bulk of the html, leaving\n",
    "# only the html for the table, which is still alot but narrowing it down. We could see the html for the table by running\n",
    "# standings_table \n",
    "\n",
    "standings_table = soup.select('table.stats_table')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next thing we need to do is to find all of the a tags inside our table so what we can do is:\n",
    "\n",
    "links = standings_table.find_all('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next we want to get the href property of each link so what we'll do is write a list comprehension that says:\n",
    "\n",
    "links = [l.get(\"href\") for l in links]\n",
    "\n",
    "# what this does is it goes through each of the a elements and then it finds the value of the href property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the next thing we need to do is filter our links so we only have the squad links using another list comprehension\n",
    "\n",
    "links = [l for l in links if '/squads/' in l]\n",
    "\n",
    "# what this will do is basically say is squad in the link, and if it isn't get rid of the link.\n",
    "# next type links in another code and run it and see what you have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_urls = [f\"https://fbref.com{l}\" for l in links]\n",
    "\n",
    "# what this will do is basically take each of our links and add the string above to the beginning of that link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXTRACT MATCH STATS USING PANDAS AND REQUESTS:\n",
    "\n",
    "# now what we can do is actually start getting the stats we want from one of these team urls, so for now we'll just work with the first\n",
    "# team url.\n",
    "\n",
    "team_url = team_urls[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now what we can do is again use requests to get the html from that url:\n",
    "\n",
    "data = requests.get(team_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next we can import pandas that will make what we need \"scores and fixures\" from our team page and pandas will turn it into a dataset\n",
    "# for us so: we're going to turn that match table from fbref.com into a pandas data frame using the pandas read html method\n",
    "\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what \"match\" does is it looks for a specific string inside the table and what \"pd.read_html\" does is it reads all of the tables\n",
    "# on the page so essentially we're scanning all of the tables on the page, all of the table tags and then we're looking for one\n",
    "# that has this string inside of it \"match=\"Scores & Fixtures\"\n",
    "\n",
    "matches = pd.read_html(data.text, match=\"Scores & Fixtures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET MATCH SHOOTING STATS WITH REQUESTS AND PANDAS\n",
    "\n",
    "# the first thing we must do if we want to get the data involving the shooting stats is get the url of the shooting stats. \n",
    "# we're going to use beautiful soup, so we'll initialize a beautiful soup instance and pass in our html\n",
    "\n",
    "\n",
    "soup = BeautifulSoup(data.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next we'll find all the links on the page\n",
    "\n",
    "links = soup.find_all('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next what we'll do is use a list comprehension to get the actual url of the link\n",
    "\n",
    "links = [l.get(\"href\") for l in links]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then we'll use another list comprehension to filter our links and only find the link to the shooting stats\n",
    "\n",
    "links = [l for l in links if l and 'all_comps/shooting/' in l]\n",
    "\n",
    "# so what this will do is we're looking for any links that have this that element in them \"all_comp/shooting/\" are the shooting links\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what we'll do now is go ahead and grab the html for this specific link\n",
    "\n",
    "data = requests.get(f\"https://fbref.com{links[0]}\")\n",
    "\n",
    "# this will download our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can do something fairly similar to before using pandas to actually read in our shooting stats \n",
    "\n",
    "shooting = pd.read_html(data.text, match=\"Shooting\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLEANING AND MERGING SCRAPED DATA WITH PANDAS\n",
    "\n",
    "shooting.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "shooting.columns = shooting.columns.droplevel()\n",
    "\n",
    "# this will just drop the top index level for us and then if we run shooting.head() again we see that index level is gone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shooting.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can do something like:\n",
    "\n",
    "shooting[\"Date\"]\n",
    "\n",
    "# which will just give us the date column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or we could do something like:\n",
    "\n",
    "shooting[\"Result\"]\n",
    "\n",
    "# which will give us the results for that week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we have two different data frames, we have the shooting data and match data data frame. Now we need to combine these two\n",
    "# What we're gonna use is the pandas merge method to merge these data frames together. we'll assign that to a variable called team data.\n",
    "\n",
    "team_data = matches[0].merge(shooting[[\"Date\", \"Sh\", \"SoT\", \"Dist\", \"FK\", \"PK\", \"PKatt\"]], on=\"Date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_data.head()\n",
    "\n",
    "# now we can see that we now have both of our data frames merged together. So we've essentially taken the matches dataframe\n",
    "# and added a few extra columns to the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So what we have accomplished here is we have gone ahead and scraped the standings and then we have downloaded the data for a \n",
    "# single team and we've combined the data for a single team in a single season into one data frame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCRAPING DATA FOR MULTIPLE SEASON AND TEAMS WITH A LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = list(range(2025, 2023, -1)) # this will scrape 2024 - 2025 season and 2023 - 2024 season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next we'll initizalize a list called all matches, what this list will contain when our loop is finished is several data frames,\n",
    "# and each data frame is going to contain the match logs for one team in one season so we'll end up  a bunch of little data frames\n",
    "# that will then combine into one big data frame at the end once our loop is finished. \n",
    "\n",
    "\n",
    "all_matches = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then we need to define the url that we want to start on so this url is going to be the same url we used initially.\n",
    "# we're going to add another layer to this and what we'll do in addition is we'll actually go to previous seasons and scrape \n",
    "# those seasons as well. which we will start in the next cell block.\n",
    "\n",
    "standings_url = \"https://fbref.com/en/comps/11/Serie-A-Stats\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "for year in years:\n",
    "    data = requests.get(standings_url) # 1st we will get the standings url html\n",
    "    soup = BeautifulSoup(data.text)  # 2nd we will get beautfiul soup to parse that html file\n",
    "    standings_table = soup.select('table.stats_table') [0] # 3rd we will get out stats table whivh contains all of our individual team links which give us the individual match data for each team.\n",
    "    \n",
    "    links = [l.get(\"href\") for l in standings_table.find_all('a')] # 4th find all of the team links and get the href property\n",
    "    links = [l for l in links if '/squads/' in l] # 5th then we'll filter the links so we only have the links for the squads\n",
    "    team_urls = [f\"https://fbref.com{l}\" for l in links] # 6th then what we'll do is we will turn these from relative links into absolute links\n",
    "    \n",
    "    previous_season = soup.select(\"a.prev\")[0].get(\"href\") # we need to grab the url for the previous season because we will be scraping information from previous seasons\n",
    "    standings_url = f\"https://fbref.com{previous_season}\" # then we will convert that into an absolute url\n",
    "    \n",
    "    for team_url in team_urls: #7th then what we need to do is loop through each of the team urls and what we'll do is individually scrape the match logs for each team\n",
    "        team_name = team_url.split(\"/\")[-1].replace(\"-Stats\", \"\").replace(\"-\", \" \") # 8th what this line does is removes everything before the \"/\" giving us only the team name, and the replace removes anything we dont want in this case a dash and the word stats after the team name\n",
    "        \n",
    "        data = requests.get(team_url) #9th the next thing we'll do is get that team url which will let us get that scores and fixtures table and we'll read that into matches\n",
    "        matches = pd.read_html(data.text, match=\"Scores & Fixtures\") [0]\n",
    "        \n",
    "        soup = BeautifulSoup(data.text) # 11th next we'll need to parse that code\n",
    "        links = [l.get(\"href\") for l in soup.find_all('a')]\n",
    "        links = [l for l in links if l and 'all_comps/shooting/' in l]\n",
    "        \n",
    "         # so what we did here in the 4 lines above is we're first parsing the socres and fixtures table, then we're pulling out the all comps shooting link\n",
    "         # because that will let us get the shooting stats, \n",
    "     \n",
    "         # then we convert that to an absolute url \n",
    "        \n",
    "        \n",
    "        data = requests.get(f\"https://fbref.com{links[0]}\")\n",
    "         \n",
    "        \n",
    "        # then what we do is we read in our shooting stats using pandas\n",
    "        \n",
    "        \n",
    "        shooting = pd.read_html(data.text, match=\"Shooting\") [0]\n",
    "        shooting.columns = shooting.columns.droplevel()\n",
    "        \n",
    "        # next what we'll do is merge our shooting stats with our match stats \n",
    "        \n",
    "        try:\n",
    "            team_data = matches.merge(shooting[[\"Date\", \"Sh\", \"SoT\", \"Dist\", \"FK\", \"PK\", \"PKatt\"]], on=\"Date\")\n",
    "        except ValueError:\n",
    "            continue\n",
    "        \n",
    "        \n",
    "         # so sometimes for some teams the shooting stats aren't available and when you try to actually merge the two together \n",
    "         # pandas gives you a value error because the shooting stats is empty shooting stats data frame is empty so in these cases \n",
    "         # what we're going to do is just ignore that team so this says hey try to merge the team data together if pandas has an error \n",
    "         # which in this case is a specific kind of error called a value error if pandas has a value error then just continue with the loop\n",
    "         # and don't do anything else so we're essentially skipping over any teams where the shooting stats aren't available   \n",
    "        \n",
    "         # Next what we'll do is we'll filter this so it only shows competitions that took place in the leagie and not champions league or \n",
    "         # domestic cup games or friendlies, ONLY Serie A GAMES.\n",
    "        \n",
    "        \n",
    "        team_data = team_data[team_data[\"Comp\"] == \"Serie A\"]\n",
    "        \n",
    "        # next we have to add in season and team columns because we're scraping this table out we're going to be combining this with \n",
    "        # alot of other tables for other teams and we need a way to distinguish okay which team was this actually for and which season \n",
    "        # was this for so that's why we're adding in some extra columns here that show us that all right so that's why we're adding \n",
    "        # this team and season column this is really something to be aware of when you're web scraping.\n",
    "        \n",
    "        \n",
    "        team_data[\"Season\"] = year\n",
    "        team_data[\"Team\"] = team_name\n",
    "        \n",
    "        # the next thing we'll do is have this list all_matches which is going to be a list of data frames and we're just going to add \n",
    "        # this team_data frame to that list\n",
    "        \n",
    "        \n",
    "        all_matches.append(team_data)\n",
    "        \n",
    "        \n",
    "        # the final thing we'll do is we'll sleep for a second, the reason we're doing this is because a lot of websites including fbref \n",
    "        # allow scraping but don't want you to scrape too quickly because it can slow down their website and make it hard for their \n",
    "        # website to run effectively so by slowing down how quickly we're scraping we're making sure we don't get blocked from scraping \n",
    "        # the website really important to do that it's a nice thing to do for the people who own the site so you're not making a lot of \n",
    "        # requests very quickly\n",
    "        \n",
    "        \n",
    "        time.sleep(1)\n",
    "        \n",
    "        # this is the loop, and there are a couple things we must do after the loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the first thing we need to do is combine all of our indicidual data frames into one data frame. We'll use the concat function in\n",
    "# pandas to do this, so it takes a list of data frames as input and returns a single data frame.\n",
    "\n",
    "match_df = pd.concat(all_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# another thing we can do, but is not necessary is making all the column names lowercase whatever your preference may be\n",
    "\n",
    "match_df.columns = [c.lower() for c in match_df.columns]\n",
    "\n",
    "# so what this will do is it will go through all the columns in this match \n",
    "# data frame lowercase them and then assign them back so it'll replace all the column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the final thing we want to do is write this to csv using the pandas to csv method so this will write all of our data to a csv file\n",
    "# called matches.csv\n",
    "\n",
    "match_df.to_csv(\"matches.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
